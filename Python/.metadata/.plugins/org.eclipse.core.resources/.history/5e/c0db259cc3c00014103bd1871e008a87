from neurons import *
from connections import *
from neuron import Neuron
from connection import Connection
class Network:
    def __init__(self, layers, connectionType = None, activations = None):
        
        if( connectionType == None ):
            connectionType = BPROPConnection 
        #TODO
        if( activations == None):
            pass
        if (len(layers) < 2):
            raise AttributeError("Not enough layers specified")
        if(len(activations) != len(layers)):
            raise AttributeError("Activation functions do not line up to layers")
        self.activations = activations
        self.bias = BiasNeuron()
        layerCount = len(layers)
        
        self.neurons = list(layerCount)
        for layer in range(0, layerCount):
            count = layers[layer]
            
            self.neurons[layer] = Neuron[count]
            
            for k in range(0, count):
                if(layer == 0):
                    self.neurons[layer][k] = InputNeuron()
                elif(layer == layerCount -1):
                    self.neurons[layer][k] = OutputNeuron()
                else:
                    self.neurons[layer][k] = Neuron()
        connections = list(layerCount-1)
        for layer in range(0, layerCount-1):
            count = (len(self.neurons[layer])+1)*len(self.neurons[layer+1])
            connections[layer] = Connection[count]
            for i in range(0, len(self.neurons[layer])+1):
                for j in range(0, len(self.neurons[layer+1])):
                    con = j + i*len(self.neurons[layer+1])
                    if i == 0:
                        connections[layer][con] = connectionType(self.bias, self.neurons[layer+1][j])
                    else: 
                        connections[layer][con] = connectionType(self.neurons[layer][i-1], self.neurons[layer+1][j])
                    connections[layer][con].network = self
    def train(self, ds, learningParameters):
        setError=0
        for dp in ds:
            self.feedForward(dp.input)
            self.backPropagate(dp.desired)
            setError+= self.globalError
        self.globalError = setError
        print(self.globalError)
        self.updateWeights(learningParameters)
        return setError
    #TODO
    def save(self, filename):
         pass
    def load(self, filename, type):
         pass
    def feedForward(self, inputs):
        if(len(inputs)!=len(self.neurons[0])):
            raise AttributeError("Input is not the same length as the neural input")
        
        for layer in self.neurons:
            for n in layer:
                n.reset()
        for i in range(0, len(inputs)):
            self.neurons[0][i].net = inputs[i]
        self.bias.updateOutput(Sigmoid.hyperbolicTangent())
        for layer in range(0, len(self.neurons))
                    
            